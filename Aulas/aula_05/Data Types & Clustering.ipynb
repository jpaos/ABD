{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Types and Aggregations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this lesson we will work with different datatypes of data and we will perform some aggregations on data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "- <a href='#1'>1. Context and Motivation</a>\n",
    "- <a href='#2'>2. Working with different Data Types</a>\n",
    "    - <a href='#2.1'>2.1. Booleans</a>\n",
    "    - <a href='#2.2'>2.2. Numbers</a>\n",
    "    - <a href='#2.3'>2.3. Strings</a>\n",
    "    - <a href='#2.4'>2.4. Dates and timestamps</a>\n",
    "    - <a href='#2.5'>2.5. Null values</a>\n",
    "    - <a href='#2.6'>2.6. Complex Types </a>\n",
    "    - <a href='#2.7'>2.7. User Defined functions</a>\n",
    "- <a href='#4'>4.  Exercises</a>\n",
    "    - <a href='#4.1'>4.1. EDA</a>\n",
    "    - <a href='#4.2'>4.2. Clustering</a>\n",
    "    - <a href='#4.3'>4.3. Evaluation</a>\n",
    "- <a href='#5'>5.  References</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <a id='1'>1. Context and Motivation</a>\n",
    "\n",
    "When we have a dataset with multiple data types, we need work with the column types specified in the dataset to do that we need to know how to work with datatypes in spark.\n",
    "\n",
    "Aggregation is an act of collecting something together. We need to perform aggregations to transform one or more columns, grouping data and view the data the way we want."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <a id='2'>2. Working with different Data Types</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this chapter we will continue analyse and transform a **retail dataset**. \n",
    "\n",
    "The file must be on hdfs however if not you have to put it there using these instructions:\n",
    "* Open a new terminal. \n",
    "* Run `hdfs dfs -put retail_data_2010-12-01.csv`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Read the dataset \n",
    "df = spark.read.format(\"csv\")\\\n",
    ".option(\"header\", \"true\")\\\n",
    ".option(\"inferSchema\",\"true\")\\\n",
    ".load(\"retail_data_2010-12-01.csv\")\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.createOrReplaceTempView(\"dfTable\") # create a new view for the data is like a view in sql "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lit function\n",
    "\n",
    "`pyspark.sql.functions.lit(col)`\n",
    "* Creates a Column of literal value. \n",
    "* See https://spark.apache.org/docs/2.1.0/api/python/pyspark.sql.html#module-pyspark.sql.functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import lit\n",
    "df.select(lit(5), lit(\"five\"), lit(5.0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='2.1'>2.1. Booleans</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Booleans are the foundation of all **filtering**.  \n",
    "Boolean statements consist of four elements: **and, or, true, and false**.   \n",
    "We use these simple structures to build logical statements that evaluate to either **true or false**. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# in Python\n",
    "from pyspark.sql.functions import col\n",
    "df.where(col(\"InvoiceNo\") != 536365)\\\n",
    ".select(\"InvoiceNo\", \"Description\")\\\n",
    ".show(5, True) # What happens when this parameter is True or False? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.where(\"InvoiceNo <> 536365\").show(5, True) # Same way to express difference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.where(\"InvoiceNo <> 536365\").explain() # Same physical plan will return same results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.where(col(\"InvoiceNo\") != 536365)\\\n",
    ".select(\"InvoiceNo\", \"Description\").explain()# Same physical plan will return same results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function **instr(str, substr)** \n",
    "* Returns the (1-based) index of the first occurrence of substr in str. \n",
    "* See https://spark.apache.org/docs/2.3.0/api/sql/index.html#instr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import instr\n",
    "priceFilter = col(\"UnitPrice\") > 600\n",
    "descripFilter = instr(df.Description, \"POSTAGE\") >= 1\n",
    "df.where(df.StockCode.isin(\"DOT\")).where(priceFilter | descripFilter).show() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.where(df.StockCode.isin(\"DOT\")).where(priceFilter | descripFilter).explain()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DOTCodeFilter = col(\"StockCode\") == \"DOT\" # Filter if StockCode contains \"DOT\"\n",
    "priceFilter = col(\"UnitPrice\") > 600 # Filter if UnitPrice has more than 600\n",
    "descripFilter = instr(col(\"Description\"), \"POSTAGE\") >= 1 # Filter if Description has POSTAGE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.withColumn(\"isExpensive\", DOTCodeFilter & (priceFilter | descripFilter))\\\n",
    ".where(\"isExpensive\")\\\n",
    ".select(\"unitPrice\", \"isExpensive\").show(5) # Creates a new column is expensive based on the filters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.withColumn(\"isExpensive\", DOTCodeFilter & (priceFilter | descripFilter))\\\n",
    ".where(\"isExpensive\")\\\n",
    ".select(\"unitPrice\", \"isExpensive\").explain()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DOTCodeFilter = col(\"StockCode\") == \"DOT\" # Filter if StockCode contains \"DOT\"\n",
    "\n",
    "df.withColumn(\"isExpensive\", DOTCodeFilter)\\\n",
    ".where(\"isExpensive\")\\\n",
    ".select(\"unitPrice\", \"isExpensive\").show(5) # Creates a new column is expensive based on the filters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "expr(\"NOT UnitPrice <= 250\") # Check the expression before apply it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import expr\n",
    "df.withColumn(\"isExpensive\", expr(\"NOT UnitPrice <= 500\"))\\\n",
    ".where(\"isExpensive\")\\\n",
    ".select(\"Description\", \"UnitPrice\").show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='2.2'>2.2. Numbers</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The second most common task you will do after filtering things is counting things. For the most part, we simply need to express our computation, and that should be valid assuming that we’re working with numerical data types."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function **pow(expr1, expr2)** \n",
    "* Raises expr1 to the power of expr2. \n",
    "* See https://spark.apache.org/docs/2.3.0/api/sql/#pow "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import pow\n",
    "\n",
    "fabricatedQuantity = pow(col(\"Quantity\") * col(\"UnitPrice\"), 2) + 5\n",
    "df.select(expr(\"CustomerId\"), fabricatedQuantity.alias(\"realQuantity\")).show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.selectExpr(\"CustomerId\",\"(POWER((Quantity * UnitPrice), 2.0) + 5) as realQuantity\").show(2) # Using sql"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.selectExpr(\"CustomerId\",\"(POWER((Quantity * UnitPrice), 2.0) + 5) as realQuantity\").explain()\n",
    "df.select(expr(\"CustomerId\"), fabricatedQuantity.alias(\"realQuantity\")).explain()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function **round(expr, d)** \n",
    "* Returns expr rounded to d decimal places using HALF_UP rounding mode. \n",
    "* https://spark.apache.org/docs/2.3.0/api/sql/#round\n",
    "\n",
    "Function **bround(expr, d)** \n",
    "* Returns expr rounded to d decimal places using HALF_EVEN rounding mode.. \n",
    "* https://spark.apache.org/docs/2.3.0/api/sql/#bround"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import lit, round, bround"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.select(round(lit(\"2.5\")), bround(lit(\"2.5\"))).show(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Function **corr(col1, col2, method=None)**   \n",
    "Indicates the correlation between two columns. The output is between -1 and 1.\n",
    "* -1 indicates lower correlation\n",
    "* 1 indicates higher correlation\n",
    "https://spark.apache.org/docs/2.2.0/api/python/pyspark.sql.html\n",
    "https://www.spss-tutorials.com/pearson-correlation-coefficient/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import corr\n",
    "print(df.stat.corr(\"Quantity\", \"UnitPrice\")) # \n",
    "df.select(corr(\"Quantity\", \"UnitPrice\")).show() # Check if cheaper things are brought together. What is the answer?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import count, mean, stddev_pop, min, max  # same as describe function one by one"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function **approxQuantile(col, probabilities, relativeError)** \n",
    "* Calculates the approximate quantiles of numerical columns of a DataFrame. \n",
    "* https://spark.apache.org/docs/2.2.0/api/python/pyspark.sql.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "colName = \"UnitPrice\"\n",
    "quantileProbs = [0.5]\n",
    "relError = 0.05\n",
    "df.stat.approxQuantile(\"UnitPrice\", quantileProbs, relError)  # Dataframe method not needed to be imported"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function **freqItems(cols, support=None)** \n",
    "* Finding frequent items for columns. \n",
    "* https://spark.apache.org/docs/2.2.0/api/python/pyspark.sql.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.stat.freqItems([\"StockCode\", \"Quantity\"],0.1).show(100000000,False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function **monotonically_increasing_id** \n",
    "* Returns monotonically increasing 64-bit integers. \n",
    "* https://spark.apache.org/docs/2.3.0/api/sql/#monotonically_increasing_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import monotonically_increasing_id\n",
    "df.select(monotonically_increasing_id()).show(2) # create indices for the dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='2.3'>2.3. Strings</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function **initcap(str)** \n",
    "* Returns str with the first letter of each word in uppercase.\n",
    "* https://spark.apache.org/docs/2.3.0/api/sql/#initcap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import initcap\n",
    "df.select(initcap(col(\"Description\"))).show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import lower, upper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.select(col(\"Description\"),\n",
    "lower(col(\"Description\")),\n",
    "upper(lower(col(\"Description\")))).show(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **ltrim** -> https://spark.apache.org/docs/2.3.0/api/sql/#ltrim  \n",
    "* **rtrim** -> https://spark.apache.org/docs/2.3.0/api/sql/#rtrim  \n",
    "* **lpad** -> https://spark.apache.org/docs/2.3.0/api/sql/#lpad  \n",
    "* **trim** -> https://spark.apache.org/docs/2.3.0/api/sql/#trim  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import ltrim, rtrim, rpad, lpad, trim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.select(\n",
    "ltrim(lit(\" HELLO \")).alias(\"ltrim\"),\n",
    "rtrim(lit(\" HELLO \")).alias(\"rtrim\"),\n",
    "trim(lit(\" HELLO \")).alias(\"trim\"),\n",
    "lpad(lit(\"HELLO\"), 3, \" \").alias(\"lp\"),\n",
    "rpad(lit(\"HELLO\"), 10, \" \").alias(\"rp\")).show(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Regular Expressions**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Probably one of the most frequently performed tasks is searching for the existence of one string\n",
    "in another or replacing all mentions of a string with another value.   \n",
    "This is done using `Regular Expressions`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function **regexp_replace(str, regexp, rep)** \n",
    "* Replaces all substrings of str that match regexp with rep.\n",
    "* See https://spark.apache.org/docs/2.3.0/api/sql/#regexp_replace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import regexp_replace\n",
    "regex_string = \"BLACK|WHITE|RED|GREEN|BLUE\"\n",
    "df.select(regexp_replace(col(\"Description\"), regex_string, \"COLOR\").alias(\"color_clean\"),col(\"Description\")).show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function **translate(input, from, to)** \n",
    "* Translates the input string by replacing the characters present in the from string with the corresponding characters in the to string. \n",
    "* See https://spark.apache.org/docs/2.3.0/api/sql/#translate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import translate\n",
    "df.select(translate(col(\"Description\"), \"LEET\", \"1337\"),col(\"Description\"))\\\n",
    ".show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "containsBlack = instr(col(\"Description\"), \"BLACK\") >= 1\n",
    "#containsWhite = instr(col(\"Description\"), \"WHITE\") >= 1\n",
    "df.withColumn(\"hasSimpleColor\", containsBlack | containsWhite)\\\n",
    ".where(\"hasSimpleColor\")\\\n",
    ".select(\"Description\").show(5, False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function **locate(substr, str[, pos])** \n",
    "* Returns the position of the first occurrence of substr in str after position pos.\n",
    "* See https://spark.apache.org/docs/2.3.0/api/sql/#locate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import expr, locate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def color_locator(column, color_string):\n",
    "    return locate(color_string.upper(), column).cast(\"boolean\").alias(\"is_\" + color_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "simpleColors = [\"black\", \"white\", \"red\", \"green\", \"blue\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "selectedColumns = [color_locator(df.Description, c) for c in simpleColors]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "selectedColumns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "selectedColumns.append(expr(\"*\")) # has to a be Column type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "selectedColumns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.select(*selectedColumns).where(expr(\"is_white OR is_red\"))\\\n",
    ".select(\"Description\").show(20, False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='2.4'>2.4. Working with Dates and Timestamps</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dates and Timestamps has many challenges.  \n",
    "It’s always necessary to keep track of timezones and ensure that formats are correct and valid.\n",
    "\n",
    "Spark has **dates**, which focus exclusively on calendar dates, and **timestamps**, which include both date\n",
    "and time information.\n",
    "\n",
    "Spark’s TimestampType class supports only second-level precision, which means that if\n",
    "we are going to be working with milliseconds or microseconds, we will need to work around this\n",
    "problem by potentially operating on them as longs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function **current_date()** \n",
    "* Returns the current date at the start of query evaluation.\n",
    "* See https://spark.apache.org/docs/2.3.0/api/sql/#current_date\n",
    "\n",
    "Function **current_timestamp()** \n",
    "* Returns the current timestamp at the start of query evaluation.\n",
    "* See https://spark.apache.org/docs/2.3.0/api/sql/#current_timestamp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import current_date, current_timestamp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# in Python\n",
    "dateDF = spark.range(10)\\\n",
    ".withColumn(\"today\", current_date())\\\n",
    ".withColumn(\"now\", current_timestamp())# creates a Dataframe with ID, Current Date and current timestamp with 10 equal entries\n",
    "dateDF.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dateDF.show(5,False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function **date_add(start_date, num_days)** \n",
    "* Returns the date that is num_days after start_date.\n",
    "* See https://spark.apache.org/docs/2.4.0/api/sql/#date_add\n",
    "\n",
    "Function **date_sub(start_date, num_days)** \n",
    "* Returns the date that is num_days before start_date.\n",
    "* See https://spark.apache.org/docs/2.4.0/api/sql/#date_sub\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import date_add, date_sub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dateDF.select(date_sub(col(\"today\"), 5), date_add(col(\"today\"), 5)).show(1) # Add and subtract 5 days"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function **datediff(endDate, startDate)** \n",
    "* Returns the number of days from startDate to endDate.\n",
    "* See https://spark.apache.org/docs/2.3.0/api/sql/#datediff\n",
    "    \n",
    "Function **months_between(timestamp1, timestamp2)** \n",
    "* Returns number of months between timestamp1 and timestamp2.\n",
    "* See https://spark.apache.org/docs/2.3.0/api/sql/#months_between\n",
    "  \n",
    "Function **to_date(date_str[, fmt])** \n",
    "* Parses the date_str expression with the fmt expression to a date. Returns null with invalid input. By default, it follows casting rules to a date if the fmt is omitted.\n",
    "* See https://spark.apache.org/docs/2.3.0/api/sql/#to_date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import datediff, months_between, to_date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dateDF.withColumn(\"week_ago\", date_sub(col(\"today\"), 7)).select(datediff(col(\"week_ago\"), col(\"today\"))).show(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dateDF.select(\n",
    "to_date(lit(\"2018-12-02\")).alias(\"end\"),\n",
    "to_date(lit(\"2019-05-22\")).alias(\"start\"))\\\n",
    ".select(months_between(col(\"start\"), col(\"end\"))).show(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dateDF.select(to_date(lit(\"2018-20-12\")),to_date(lit(\"2018-12-11\"))).show(1) # spark will not return an error instead create a null value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dateFormat = \"yyyy-dd-MM\"\n",
    "cleanDateDF = spark.range(1).select(\n",
    "to_date(lit(\"2018-12-11\"), dateFormat).alias(\"date\"),\n",
    "to_date(lit(\"2018-20-12\"), dateFormat).alias(\"date2\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleanDateDF.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert into timestamp \n",
    "from pyspark.sql.functions import to_timestamp\n",
    "cleanDateDF.select(to_timestamp(col(\"date\"), dateFormat)).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleanDateDF.filter(col(\"date2\") > lit(\"2018-12-12\")).show() # compare dates"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='2.5'>2.5. Nulls Values</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As best practice we should always Sometimes we get null values to let know spark how to handle it internally"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import coalesce\n",
    "df.select(coalesce(col(\"Description\"), col(\"CustomerId\"))).show(20,False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Removes rows that contain nulls. \n",
    "#The default is to drop any row in which any value is null:\n",
    "df.na.drop() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Drops a row if any of the values are null.\n",
    "df.na.drop(\"any\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drops the row only if all values are null or NaN for that row\n",
    "df.na.drop(\"all\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Delete the rows which has null values in StockCode and InvoceNo simultaneously\n",
    "df.na.drop(\"all\", subset=[\"StockCode\", \"InvoiceNo\"]) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fill   \n",
    "Using the fill function, you can fill one or more columns with a set of values.    \n",
    "This can be done by specifying a map—that is a particular value and a set of columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.na.fill(\"All Null values become this string\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.na.fill(\"all\", subset=[\"StockCode\", \"InvoiceNo\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fill_cols_vals = {\"StockCode\": 5, \"Description\" : \"No Value\"}\n",
    "df.na.fill(fill_cols_vals).show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Replace\n",
    "Using the fill function, you can fill one or more columns with a set of values.    \n",
    "This can be done by specifying a map—that is a particular value and a set of columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.na.replace([\"\"], [\"UNKNOWN\"], \"Description\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='2.6'>2.6. Complex Types</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.select(split(col(\"Description\"), \" \")).show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.select(split(col(\"Description\"), \" \").alias(\"array_col\"))\\\n",
    ".selectExpr(\"array_col[0]\").show(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Array Length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.select(size(split(col(\"Description\"), \" \"))).show(2) # shows 5 and 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Array_contains"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import array_contains"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.select(array_contains(split(col(\"Description\"), \" \"), \"WHITE\")).show(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Explode\n",
    "\n",
    "The explode function takes a column that consists of arrays and creates one row (with the rest of\n",
    "the values duplicated) per value in the array. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import split, explode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.withColumn(\"splitted\", split(col(\"Description\"), \" \"))\\\n",
    ".withColumn(\"exploded\", explode(col(\"splitted\")))\\\n",
    ".select(\"Description\", \"InvoiceNo\", \"exploded\").show(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Maps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import create_map\n",
    "df.select(create_map(col(\"Description\"), col(\"InvoiceNo\")).alias(\"complex_map\")).show(5,False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.select(create_map(col(\"Description\"), col(\"InvoiceNo\")).alias(\"complex_map\"))\\\n",
    ".selectExpr(\"complex_map['WHITE METAL LANTERN']\").show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.select(create_map(col(\"Description\"), col(\"InvoiceNo\")).alias(\"complex_map\"))\\\n",
    ".selectExpr(\"explode(complex_map)\").show(5,False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='2.7'>2.7. User Defined functions</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Spark will serialize the function on the driver and transfer it over the network to all executor processes.\n",
    "Spark starts a Python process on the worker, serializes all of the data to a format that Python can understand, executes the function row by row on that data in the Python process, and then finally returns the results of the row operations to the JVM and Spark."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "udfExampleDF = spark.range(5).toDF(\"num\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def power3(double_value):\n",
    "    return double_value ** 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import udf\n",
    "power3udf = udf(power3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col\n",
    "udfExampleDF.select(power3udf(col(\"num\"))).show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import IntegerType, DoubleType\n",
    "spark.udf.register(\"power3py\", power3, IntegerType()) # register the function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the function as SQL expression "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "udfExampleDF.selectExpr(\"power3py(num)\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <a id='4'>4. Exercises</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Customer Churn\n",
    "\n",
    "Customer churn, also known as customer attrition, customer turnover, or customer defection, is the loss of clients or customers.\n",
    "\n",
    "Telephone service companies, Internet service providers, pay TV companies, insurance firms, and alarm monitoring services, often use customer churn analysis and customer churn rates as one of their key business metrics because the cost of retaining an existing customer is far less than acquiring a new one. Companies from these sectors often have customer service branches which attempt to win back defecting clients, because recovered long-term customers can be worth much more to a company than newly recruited clients.\n",
    "\n",
    "Companies usually make a distinction between voluntary churn and involuntary churn. Voluntary churn occurs due to a decision by the customer to switch to another company or service provider, involuntary churn occurs due to circumstances such as a customer's relocation to a long-term care facility, death, or the relocation to a distant location. In most applications, involuntary reasons for churn are excluded from the analytical models. Analysts tend to concentrate on voluntary churn, because it typically occurs due to factors of the company-customer relationship which companies control, such as how billing interactions are handled or how after-sales help is provided.\n",
    "\n",
    "Predictive analytics use churn prediction models that predict customer churn by assessing their propensity of risk to churn. Since these models generate a small prioritized list of potential defectors, they are effective at focusing customer retention marketing programs on the subset of the customer base who are most vulnerable to churn."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Column Description   \n",
    "\n",
    "| Column     | Type       | Description |\n",
    "|--------  |---------  |: --------- |\n",
    "| **customerID** | String | Customer ID |\n",
    "| **gender** | String | Whether the customer is a male or a female |\n",
    "| **SeniorCitizen** | Integer | Whether the customer is a senior citizen or not (1, 0) |\n",
    "| **Partner** | String | Whether the customer has a partner or not (Yes, No) |\n",
    "| **Dependents** | String | Whether the customer has dependents or not (Yes, No) |\n",
    "| **tenure** | Integer | Number of months the customer has stayed with the company |\n",
    "| **PhoneService** | String | Whether the customer has a phone service or not (Yes, No) |\n",
    "| **MultipleLines** | String | Whether the customer has multiple lines or not (Yes, No, No phone service) |\n",
    "| **InternetService** | String | Customer’s internet service provider (DSL, Fiber optic, No) |\n",
    "| **OnlineSecurity** | String | Whether the customer has online security or not (Yes, No, No internet service) |\n",
    "| **OnlineBackup** | String | Whether the customer has online backup or not (Yes, No, No internet service) |\n",
    "| **DeviceProtection** | String | Whether the customer has device protection or not (Yes, No, No internet service) |\n",
    "| **TechSupport** | String | Whether the customer has tech support or not (Yes, No, No internet service) |\n",
    "| **StreamingTV** | String | Whether the customer has streaming movies or not (Yes, No, No internet service) |\n",
    "| **StreamingMovies** | String | Whether the customer has a partner or not (Yes, No) |\n",
    "| **Contract** | String | The contract term of the customer (Month-to-month, One year, Two year) |\n",
    "| **PaperlessBilling** | String | Whether the customer has paperless billing or not (Yes, No) |\n",
    "| **PaymentMethod** | String | The customer’s payment method (Electronic check, Mailed check, Bank transfer (automatic), Credit card (automatic)) |\n",
    "| **MonthlyCharges** | Double | The amount charged to the customer monthly |\n",
    "| **TotalCharges** | String | The total amount charged to the customer |\n",
    "| **Churn** | String | Whether the customer churned or not (Yes or No) |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For exercises we will continue analyse the churn dataset in order to  create some **clusters** based on the information provided"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the command command `hdfs dfs -put WA_Fn-UseC_-Telco-Customer-Churn.csv` to put the dataframe in hfds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the dataframe.\n",
    "df = spark.read.format(\"csv\")\\\n",
    ".option(\"header\",\"true\")\\\n",
    ".option(\"inferSchema\",\"true\")\\\n",
    ".load(\"WA_Fn-UseC_-Telco-Customer-Churn.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='4.1'>4.1. EDA - Exploratory data analysis</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transform columns that is in strings witch don't have a description in integers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check check the relation between variables using sns pairplot\n",
    "import seaborn as sns \n",
    "sns.set(style=\"ticks\", color_codes=True)\n",
    "df = df.toPandas()\n",
    "pairplot = sns.pairplot(df, hue=\"Churn\")\n",
    "\n",
    "data_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count the number of columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count the number of rows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the min, max and metrics using describe field"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='4.2'>4.2. Clustering</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.clustering import KMeans\n",
    "va = VectorAssembler()\\\n",
    ".setInputCols([\"MonthlyCharges\",\"tenure\"])\\\n",
    ".setOutputCol(\"features\")\n",
    "\n",
    "churn = va.transform(spark.read.format(\"csv\")\\\n",
    ".option(\"header\", \"true\")\\\n",
    ".option(\"inferSchema\", \"true\")\\\n",
    ".load(\"WA_Fn-UseC_-Telco-Customer-Churn.csv\"))\n",
    "\n",
    "churn.cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "km = KMeans().setK(4)\n",
    "print (km.explainParams())\n",
    "kmModel = km.fit(churn)\n",
    "# Make predictions\n",
    "predictions = kmModel.transform(churn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='4.3'>4.3. Evaluation</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary = kmModel.summary\n",
    "print (summary.clusterSizes) # number of points\n",
    "kmModel.computeCost(churn)\n",
    "centers = kmModel.clusterCenters()\n",
    "print(\"Cluster Centers: \")\n",
    "for center in centers:\n",
    "    print(center)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.evaluation import ClusteringEvaluator\n",
    "# Evaluate clustering by computing Silhouette score\n",
    "evaluator = ClusteringEvaluator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "silhouette = evaluator.evaluate(predictions)\n",
    "print(\"Silhouette with squared euclidean distance = \" + str(silhouette))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transformed = kmModel.transform(churn).select(\"features\", \"prediction\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transformed.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <a id='5'>5. References</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://spark.apache.org/docs/2.3.0/api/sql/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://spark.apache.org/docs/2.2.0/api/python/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://spark.apache.org/docs/latest/ml-clustering.html"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
