{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Apache Spark Introduction\n",
    "In this class we talk about Spark framework and his components."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "- <a href='#1'>1. Context and Motivation</a>\n",
    "- <a href='#2'>2. Apache Spark</a>\n",
    "    - <a href='#2.1'>2.1.  Spark Components</a>\n",
    "    - <a href='#2.2'>2.2.  Spark Applications</a>\n",
    "    - <a href='#2.3'>2.3.  Spark Session</a>\n",
    "    - <a href='#2.4'>2.4.  DataFrames</a>\n",
    "    - <a href='#2.5'>2.5.  Partitions</a>\n",
    "    - <a href='#2.6'>2.6.  Transformations</a>\n",
    "    - <a href='#2.7'>2.7.  Lazy Evaluation</a>\n",
    "    - <a href='#2.8'>2.8.  Actions</a>\n",
    "    - <a href='#2.9'>2.9.  Spark UI</a>\n",
    "    - <a href='#2.10'>2.10.  SQL</a>\n",
    "- <a href='#3'>3.  Exercises</a>\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <a id='1'>1.Context and Motivation</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Why do we need spark?** \n",
    "\n",
    "Over the years computers became faster every year through processor speed increases year by year computers processes more and more information, however most of the applications was design to run only on a single processor \n",
    "The trend of faster computers every year stopped dued to hard limits. The hardware developers switch to adding more paralel CPU processing all running at the same time. This change leads to that applications needed to be modified to add paralelism in order to run faster witch set stage for new programing models such **Apache Spark**. \n",
    "\n",
    "**Apache Spark** is an open-source distributed general-purpose cluster-computing framework. Spark provides an interface for programming entire clusters with implicit data parallelism and fault tolerance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " # <a id='2'>2. Apache Spark</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## <a id='2.1'>2.1. Spark Components</a>\n",
    " Spark includes multiple components described below"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"spark_stack.png\" width=\"450px\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Spark Core\n",
    "Spark Core contains basic Spark functionalities required for running jobs and neededby  other  components.  The  most  important  of  these  is  the  resilient  distributed  dataset(RDD), which is the main element of the Spark API. It’s an abstraction of a distributed collection of items with operations and transformations applicable to the dataset. It’s resilient because it’s capable of rebuilding datasets in case of node failures."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Spark SQL\n",
    "Spark SQL  provides  functions  for  manipulating  large  sets  of  distributed,  structured data  using  an  SQL  subset  supported  by  Spark  and  Hive  SQL  (HiveQL). Spark SQL can also be used for reading and writing data to and from various structured formats and datasources, such as JavaScript Object Notation (JSON) files, Parquet files (an increasingly popular  file  format  that  allows  for  storing  a  schema  along  with  the  data), relational databases, Hive, and others.\n",
    "Operations  on  DataFrames  and  DataSets  at  some  point  translate  to  operations  on RDDs and execute as ordinary Spark jobs. Spark SQL provides a query optimization framework called Catalyst that can be extended by custom optimization rules."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Spark Streaming\n",
    "Spark  Streaming  is  a  framework  for  ingesting  real-time  streaming  data  from  various sources.  The  supported  streaming  sources  include  HDFS,  Kafka,  Flume,  Twitter,ZeroMQ,   and   custom   ones.   Spark   Streaming   operations   recover   from   failure automatically,  which  is  important  for  online  data  processing.  Spark  Streaming represents  streaming  data  using  discretized  streams (DStreams),  which  periodically create RDDs containing the data that came in during the last time window. Spark Streaming can be combined with other Spark components in a single program,unifying real-time processing with machine learning, SQL, and graph operations. This is something unique in the Hadoop ecosystem. And since Spark 2.0, the new StructuredStreaming API makes Spark streaming programs more similar to Spark batch programs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Spark MLib\n",
    "Spark MLlib is a library of machine-learning algorithms grown from the MLbase proj-ect at UC Berkeley. Supported algorithms include logistic regression, naïve Bayes clas-sification,  support  vector  machines  (SVMs),  decision  trees,  random  forests,  linearregression, and k-means clustering.  Apache Mahout is an existing open source project offering implementations of dis-tributed machine-learning algorithms running on Hadoop. Although Apache Mahoutis more mature, both Spark MLlib and Mahout include a similar set of machine-learningalgorithms. But with Mahout migrating from MapReduce to Spark, they’re bound to bemerged in the future.  Spark  MLlib  handles  machine-learning  models  used  for  transforming  datasets,which are represented as RDDs or DataFrames."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Spark GraphX\n",
    "Graphs  are  data  structures  comprising  vertices  and  the  edges  connecting  them.GraphX  provides  functions  for  building  graphs,  represented  as  graph RDDs: EdgeRDDand VertexRDD. GraphX contains implementations of the most important algorithmsof  graph  theory,  such  as  page  rank,  connected  components,  shortest  paths,  SVD++,and  others.  It  also  provides  the  Pregel  message-passing  API,  the  same  API  for  large-scale  graph  processing  implemented  by  Apache  Giraph,  a  project  with  implementa-tions of graph algorithms and running on Hadoop."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## <a id='2.2'>2.2. Spark Applications</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  <img src=\"driver_executor.png\" width=\"300px\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Spark applications consist of a **Driver Process** and a set of **executor processes**:\n",
    "\n",
    " **Driver process:**\n",
    " \n",
    "   * Run the main function.\n",
    "   * Mantain information about spark application.\n",
    "   * Responding to the user program or input. \n",
    "   * Analysing, Distributing, and scheduling work across executors.\n",
    "   \n",
    "**Executor processes:**\n",
    " \n",
    "   * Responsible for carrying out the work that driver assigns to them.\n",
    "   * Executing the code assign to it by the driver. \n",
    "   * Reporting the state of the computation on that executor back to driver node.\n",
    "   \n",
    "### Note\n",
    "There is two modes **Cluster mode** and **Local mode** In Cluster mode, Driver and executors are processes can live in the same machine or different machines. in Local mode driver and executors run(as threads) on your own computer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='2.3'>2.3. Spark Session</a>\n",
    " \n",
    "Spark SessionSession instance is the way Sparks execute user defined manipulations across the cluster.   \n",
    "To call spark Session object use `spark` command."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<pyspark.sql.session.SparkSession object at 0x115b2d710>\n"
     ]
    }
   ],
   "source": [
    "print(spark) # Spark Session Object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - hive</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://192.168.1.102:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v2.4.4</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>PySparkShell</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x115b2d710>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a data Range of numbers:\n",
    "# This range of numbers represents a distributed collection in each part of this \n",
    "# range of numbers exists on a different executor.\n",
    "numbers_to_n = spark.range(1000000000).toDF(\"Number\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## <a id='2.4'>2.4. Dataframes</a>\n",
    " Dataframe is the most common structed and simply represents  a table of data with rows and columns.  \n",
    " The list that defines columns and the types within those columns is called the schema.\n",
    " Spark Dataframes can span thousands of computers.  \n",
    " \n",
    " ### Note  \n",
    " Spark has multiple core abstractions: Datasets,Dataframes, SQL Tables and Resilient Distributed Datasets(RDD). These abstractions all represents distributed collections of Data.\n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "columns = ['id', 'dogs', 'cats']\n",
    "vals = [\n",
    "     (1, \"bulldog\", \"persian\"),\n",
    "     (2, \"German Shepherd\", \"Siamese\")\n",
    "]\n",
    "\n",
    "# create DataFrame\n",
    "df = spark.createDataFrame(vals, columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id: long (nullable = true)\n",
      " |-- dogs: string (nullable = true)\n",
      " |-- cats: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema() # see the schema"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## <a id='2.5'>2.5. Partitions</a>\n",
    " \n",
    " To allow every executor perform work in paralel, Spark breaks up the data into chuncks called partitions.\n",
    " Partition is a collection of rows that sits on one physical machine in your cluster. \n",
    " \n",
    " if we have multiple partitions but only one executor Spark will have a paralelism of only one because there is only one computation resource.  \n",
    " \n",
    " If we have one partitions spark will have a parallelism of only one even if we have a thousands of executors.  \n",
    " \n",
    " ### Note\n",
    " In Dataframes we don't (for the most part) manipulate partitions individualy, we simply specify high level transformations of data in the physical partitions, and spark determines how this work will actually execute on the cluster\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## <a id='2.6'>2.6. Transformations</a>\n",
    " \n",
    " In Spark core data structures are **Immutable**(cannot be changed after they're created).   \n",
    " To \"change\" a Dataframe we need to instruct spark to modify  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## <a id='2.7'>2.7. Lazy Evaluation</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## <a id='2.8'>2.8. Actions</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## <a id='2.9'>2.9. Spark UI</a>\n",
    " With Spark UI you can monitor  the progress of a job. Usually Spark UI is available on port 4040 of the driver node.   \n",
    " Spark displays information about the state of spark jobs, its environment and cluster state.\n",
    " It is very useful for tunning and debuging."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'spark' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-16-be81978af8bc>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msparkContext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muiWebUrl\u001b[0m \u001b[0;31m# Check where spark ui is running\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'spark' is not defined"
     ]
    }
   ],
   "source": [
    "spark.sparkContext.uiWebUrl # Check where spark ui is running"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## <a id='2.10'>2.10. SQL and DataFrames</a>\n",
    " Spark can run the same transformations regredless of the language in the exact same way.  \n",
    " Spark will compile the logic to a underlying plan."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load a dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Query in sql"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Query in sql"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sqlWay Plan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DataFrameWay Plan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## <a id='3'>3. Exercises</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Create a Dataframe that contains 100 * 100 lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
