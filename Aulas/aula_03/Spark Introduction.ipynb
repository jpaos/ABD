{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Apache Spark Introduction\n",
    "In this class we talk about Spark framework and his components."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "- <a href='#1'>1. Context and Motivation</a>\n",
    "- <a href='#2'>2. Apache Spark</a>\n",
    "    - <a href='#2.1'>2.1.  Spark Components</a>\n",
    "    - <a href='#2.2'>2.2.  Spark Applications</a>\n",
    "    - <a href='#2.3'>2.3.  Spark Session</a>\n",
    "    - <a href='#2.4'>2.4.  DataFrames</a>\n",
    "    - <a href='#2.5'>2.5.  Partitions</a>\n",
    "    - <a href='#2.6'>2.6.  Transformations</a>\n",
    "    - <a href='#2.7'>2.7.  Lazy Evaluation</a>\n",
    "    - <a href='#2.8'>2.8.  Actions</a>\n",
    "    - <a href='#2.9'>2.9.  Spark UI</a>\n",
    "    - <a href='#2.10'>2.10.  SQL</a>\n",
    "- <a href='#3'>3.  Exercises</a>\n",
    "- <a href='#3'>4.  References</a>\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <a id='1'>1.Context and Motivation</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Why do we need spark?** \n",
    "\n",
    "Over the years computers became faster every year through processor speed increases year by year computers processes more and more information, however most of the applications was design to run only on a single processor. \n",
    "The trend of faster computers every year stopped dued to hard limits. The hardware developers switch to adding more paralel CPU processing all running at the same time. This change leads to that applications needed to be modified to add paralelism in order to run faster witch set stage for new programing models such **Apache Spark**. \n",
    "\n",
    "**Apache Spark** is an open-source distributed general-purpose cluster-computing framework. Spark provides an interface for programming entire clusters with implicit data parallelism and fault tolerance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " # <a id='2'>2. Apache Spark</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## <a id='2.1'>2.1. Spark Components</a>\n",
    " Spark includes multiple components described below"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"spark_stack.png\" width=\"450px\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Spark Core\n",
    "Spark Core contains basic Spark functionalities required for running jobs and neededby  other  components.  The  most  important  of  these  is  the  resilient  distributed  dataset(RDD), which is the main element of the Spark API. It’s an abstraction of a distributed collection of items with operations and transformations applicable to the dataset. It’s resilient because it’s capable of rebuilding datasets in case of node failures."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Spark SQL\n",
    "Spark SQL  provides  functions  for  manipulating  large  sets  of  distributed,  structured data  using  an  SQL  subset  supported  by  Spark  and  Hive  SQL  (HiveQL). Spark SQL can also be used for reading and writing data to and from various structured formats and datasources, such as JavaScript Object Notation (JSON) files, Parquet files (an increasingly popular  file  format  that  allows  for  storing  a  schema  along  with  the  data), relational databases, Hive, and others.\n",
    "Operations  on  DataFrames  and  DataSets  at  some  point  translate  to  operations  on RDDs and execute as ordinary Spark jobs. Spark SQL provides a query optimization framework called Catalyst that can be extended by custom optimization rules."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Spark Streaming\n",
    "Spark  Streaming  is  a  framework  for  ingesting  real-time  streaming  data  from  various sources.  The  supported  streaming  sources  include  HDFS,  Kafka,  Flume,  Twitter,ZeroMQ,   and   custom   ones.   Spark   Streaming   operations   recover   from   failure automatically,  which  is  important  for  online  data  processing.  Spark  Streaming represents  streaming  data  using  discretized  streams (DStreams),  which  periodically create RDDs containing the data that came in during the last time window. Spark Streaming can be combined with other Spark components in a single program,unifying real-time processing with machine learning, SQL, and graph operations. This is something unique in the Hadoop ecosystem. And since Spark 2.0, the new StructuredStreaming API makes Spark streaming programs more similar to Spark batch programs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Spark MLib\n",
    "Spark MLlib is a library of machine-learning algorithms grown from the MLbase project at UC Berkeley. Supported algorithms include logistic regression, naïve Bayes classification,  support  vector  machines  (SVMs),  decision  trees,  random  forests,  linear regression, and k-means clustering. Spark  MLlib  handles  machine-learning  models  used  for  transforming  datasets,which are represented as RDDs or DataFrames."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Spark GraphX\n",
    "Graphs  are  data  structures  comprising  vertices  and  the  edges  connecting  them.GraphX  provides  functions  for  building  graphs,  represented  as  graph RDDs: EdgeRDD and VertexRDD. GraphX contains implementations of the most important algorithms of  graph  theory,  such  as  page  rank,  connected  components,  shortest  paths,  SVD, and  others."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## <a id='2.2'>2.2. Spark Applications</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  <img src=\"driver_executor.png\" width=\"300px\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Spark applications consist of a **Driver Process/program** and a set of **executor processes**:\n",
    "\n",
    " **Driver process:**\n",
    " \n",
    "   * Run the main function.\n",
    "   * Mantain information about spark application.\n",
    "   * Responding to the user program or input. \n",
    "   * Analysing, Distributing, and scheduling work across executors.\n",
    "   \n",
    "**Executor process:**\n",
    " \n",
    "   * Responsible for carrying out the work that driver assigns to them.\n",
    "   * Executing the code assign to it by the driver. \n",
    "   * Reporting the state of the computation on that executor back to driver node.\n",
    "   \n",
    "### Note\n",
    "There is two modes **Cluster mode** and **Local mode** In Cluster mode, Driver and executors are processes  that can live in the same machine or different machines. In Local mode driver and executors run(as threads) on your own computer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='2.3'>2.3. Spark Session</a>\n",
    " \n",
    "Spark SessionSession instance is the way Sparks execute user defined manipulations across the cluster.   \n",
    "To call spark Session object use `spark` command."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(spark) # Spark Session Object<<"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a data Range of numbers:\n",
    "# This range of numbers represents a distributed collection. Each part of this \n",
    "# range of numbers exists on a different executor.\n",
    "numbers_to_n = spark.range(1000000000).toDF(\"Number\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## <a id='2.4'>2.4. Dataframes</a>\n",
    " Dataframe is the most common structed and simply represents  a table of data with rows and columns.  \n",
    " The list that defines columns and the types within those columns is called the schema.\n",
    " Spark Dataframes can span thousands of computers.  \n",
    " \n",
    " ### Note  \n",
    " Spark has multiple core abstractions: Datasets,Dataframes, SQL Tables and Resilient Distributed Datasets(RDD). These abstractions all represents distributed collections of Data.\n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "columns = ['id', 'dogs', 'cats']\n",
    "vals = [\n",
    "     (1, \"bulldog\", \"persian\"),\n",
    "     (2, \"German Shepherd\", \"Siamese\")\n",
    "]\n",
    "\n",
    "# create DataFrame\n",
    "df = spark.createDataFrame(vals, columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.printSchema() # see the schema"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## <a id='2.5'>2.5. Partitions</a>\n",
    " \n",
    " To allow every executor perform work in paralel, Spark breaks up the data into chuncks called partitions.\n",
    " Partition is a collection of rows that sits on one physical machine in your cluster. \n",
    " \n",
    " If we have multiple partitions but only one executor Spark will have a paralelism of only one because there is only one computation resource.  \n",
    " \n",
    " If we have one partitions spark will have a parallelism of only one even if we have a thousands of executors.  \n",
    " \n",
    " ### Note\n",
    " In Dataframes we don't (for the most part) manipulate partitions individualy, we simply specify high level transformations of data in the physical partitions, and spark determines how this work will actually execute on the cluster\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## <a id='2.6'>2.6. Transformations</a>\n",
    " \n",
    " In Spark core data structures are **Immutable**(cannot be changed after they're created).   \n",
    " To \"change\" a Dataframe we need to instruct spark how to modify it to do what we want. these are called transformations  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "divis_by_two = numbers_to_n.where(\"number % 2 = 0\") # why didn't return the output?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Types of transformations\n",
    "* Narrow dependencies -> each input contribute only one output partition.All performed in memory.  \n",
    " E.g **Map**, **Filter**\n",
    "* Wide dependencies -> each input partitions contributing to many output partitions. Spark writes the result to disk   \n",
    " E.g **GroupByKey**, **ReduceByKey**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Narrow Transformations\n",
    "\n",
    "* **map(func)** -> Return a new distributed dataset formed by passing each element of the source through a function func.\n",
    "* **filter(func)** -> Return a new dataset formed by selecting those elements of the source on which func returns true."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "flights = sc.textFile(\"2015-summary.csv\")\n",
    "rows = flights.map(lambda line: line.split(\",\")) # Map example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rows.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rows.filter(lambda line: \"United States\" in line).collect() ## Filter Example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Wide Transformations\n",
    "* **GroupByKey(numPartitions)** -> When called on a dataset of (K, V) pairs, returns a dataset of (K, Iterable\\<V>) pairs.\n",
    "* **ReduceByKey(numPartitions)** -> When called on a dataset of (K, V) pairs, returns a dataset of (K, V) pairs where the values for each key are aggregated using the given reduce function func, which must be of type (V,V) => V. Like in groupByKey, the number of reduce tasks is configurable through an optional second argument.. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "origin = rows.map(lambda n: (n[0],n[1] )).groupByKey() # group by orgin key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "origin_and_destiny.take(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## <a id='2.7'>2.7. Lazy Evaluation</a>\n",
    " \n",
    " Lazy evaluation means that spark will wait until the very last moment to execute the graph of computation instructions.\n",
    " In spark we build a plan of transformations that we would like to apply to the data. By waiting until the last moment to execute the code. Spark compiles the plan and optimize the entire flow end to end."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "divis_by_two.explain()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## <a id='2.8'>2.8. Actions</a>\n",
    " \n",
    " Transformations allow us to build our logical transformation plan and trigger the computation. It's like the play button. \n",
    " \n",
    " ### Kinds of actions:\n",
    "* Actions to view Data in the console\n",
    "* Actions to collect data to native objects in the respective language\n",
    "* Actions to write to output data sources\n",
    "\n",
    "\n",
    " **Example Actions**:\n",
    " * **reduce(func)** -> Aggregate the elements of the dataset using a function func (which takes two arguments and returns one). The function should be commutative and associative so that it can be computed correctly in parallel.\n",
    " * **count()** -> Return the number of elements in the dataset.\n",
    " * **take(n)** -> Return an array with the first n elements of the dataset.\n",
    " * **first()** -> Return the first element of the dataset (similar to take(1)).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "divis_by_two.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "divis_by_two.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "divis_by_two.first()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## <a id='2.9'>2.9. Spark UI</a>\n",
    " With Spark UI you can monitor  the progress of a job. Usually Spark UI is available on port 4040 of the driver node.   \n",
    " Spark displays information about the state of spark jobs, its environment and cluster state.\n",
    " It is very useful for tunning and debuging."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sparkContext.uiWebUrl # Check where spark ui is running"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## <a id='2.10'>2.10. SQL and DataFrames</a>\n",
    " Spark can run the same transformations regredless of the language in the exact same way.  \n",
    " Spark will compile the logic to a underlying plan.\n",
    " \n",
    " **NOTE**  \n",
    " For the next exercises we will use fligth Data from the United states Bureau of Transportation Statistics \n",
    " Reference: https://transtats.bts.gov/ONTIME/Index.aspx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load a dataset\n",
    "# create a dataframe \n",
    "flight_data_2015 = spark.read.option(\"inferSchema\",\"true\").option(\"header\",\"true\").csv(\"2015-summary.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "flight_data_2015.take(5)# first three lines of the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "flight_data_2015.sort(\"count\").explain() ## check the spark physical plan of count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Spark by default has 200 shuffle output partitions. \n",
    "# Set partitions to 5\n",
    "spark.conf.set(\"spark.sql.shuffle.partitions\",\"5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a view of flight data \n",
    "flight_data_2015.createOrReplaceTempView(\"flight_data_2015\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SQL WAY\n",
    "country_name_sql = spark.sql(\"\"\"SELECT DEST_COUNTRY_NAME, count(1) FROM flight_data_2015 GROUP BY DEST_COUNTRY_NAME\"\"\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#DATAFRAME WAY\n",
    "country_name_sql = flight_data_2015.groupBy(\"DEST_COUNTRY_NAME\").count() # dataframe way"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "country_name_sql.explain() # same logical plan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "country_name_dataframe.explain() # same logical plan"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## <a id='3'>3. Exercises</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# What are the max count from flight data?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# What are the  top 5 destinations in data?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# what are the origin country with more flights?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# what are the destination with more flights?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='4'>4. References</a>\n",
    "https://spark.apache.org/docs/latest/rdd-programming-guide.html\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
