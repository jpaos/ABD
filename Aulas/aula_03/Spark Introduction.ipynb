{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Apache Spark Introduction\n",
    "In this class we talk about Spark framework and his components."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "- <a href='#1'>1. Context and Motivation</a>\n",
    "- <a href='#2'>2. Apache Spark</a>\n",
    "    - <a href='#2.1'>2.1.  Spark Components</a>\n",
    "    - <a href='#2.2'>2.2.  Spark Applications</a>\n",
    "    - <a href='#2.3'>2.3.  Spark Session</a>\n",
    "    - <a href='#2.4'>2.4.  DataFrames</a>\n",
    "    - <a href='#2.5'>2.5.  Partitions</a>\n",
    "    - <a href='#2.6'>2.6.  Transformations</a>\n",
    "    - <a href='#2.7'>2.7.  Lazy Evaluation</a>\n",
    "    - <a href='#2.8'>2.8.  Actions</a>\n",
    "    - <a href='#2.9'>2.9.  Spark UI</a>\n",
    "    - <a href='#2.10'>2.10.  SQL</a>\n",
    "- <a href='#3'>3.  Exercises</a>\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <a id='1'>1.Context and Motivation</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Why do we need spark?** \n",
    "\n",
    "Over the years computers became faster every year through processor speed increases year by year computers processes more and more information, however most of the applications was design to run only on a single processor \n",
    "The trend of faster computers every year stopped dued to hard limits. The hardware developers switch to adding more paralel CPU processing all running at the same time. This change leads to that applications needed to be modified to add paralelism in order to run faster witch set stage for new programing models such **Apache Spark**. \n",
    "\n",
    "**Apache Spark** is an open-source distributed general-purpose cluster-computing framework. Spark provides an interface for programming entire clusters with implicit data parallelism and fault tolerance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " # <a id='2'>2. Apache Spark</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## <a id='2.1'>2.1. Spark Components</a>\n",
    " Spark includes multiple components described below"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"spark_stack.png\" width=\"450px\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Spark Core\n",
    "Spark Core contains basic Spark functionalities required for running jobs and neededby  other  components.  The  most  important  of  these  is  the  resilient  distributed  dataset(RDD), which is the main element of the Spark API. It’s an abstraction of a distributed collection of items with operations and transformations applicable to the dataset. It’s resilient because it’s capable of rebuilding datasets in case of node failures."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Spark SQL\n",
    "Spark SQL  provides  functions  for  manipulating  large  sets  of  distributed,  structured data  using  an  SQL  subset  supported  by  Spark  and  Hive  SQL  (HiveQL). Spark SQL can also be used for reading and writing data to and from various structured formats and datasources, such as JavaScript Object Notation (JSON) files, Parquet files (an increasingly popular  file  format  that  allows  for  storing  a  schema  along  with  the  data), relational databases, Hive, and others.\n",
    "Operations  on  DataFrames  and  DataSets  at  some  point  translate  to  operations  on RDDs and execute as ordinary Spark jobs. Spark SQL provides a query optimization framework called Catalyst that can be extended by custom optimization rules."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Spark Streaming\n",
    "Spark  Streaming  is  a  framework  for  ingesting  real-time  streaming  data  from  various sources.  The  supported  streaming  sources  include  HDFS,  Kafka,  Flume,  Twitter,ZeroMQ,   and   custom   ones.   Spark   Streaming   operations   recover   from   failure automatically,  which  is  important  for  online  data  processing.  Spark  Streaming represents  streaming  data  using  discretized  streams (DStreams),  which  periodically create RDDs containing the data that came in during the last time window. Spark Streaming can be combined with other Spark components in a single program,unifying real-time processing with machine learning, SQL, and graph operations. This is something unique in the Hadoop ecosystem. And since Spark 2.0, the new StructuredStreaming API makes Spark streaming programs more similar to Spark batch programs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Spark MLib\n",
    "Spark MLlib is a library of machine-learning algorithms grown from the MLbase proj-ect at UC Berkeley. Supported algorithms include logistic regression, naïve Bayes clas-sification,  support  vector  machines  (SVMs),  decision  trees,  random  forests,  linearregression, and k-means clustering.  Apache Mahout is an existing open source project offering implementations of dis-tributed machine-learning algorithms running on Hadoop. Although Apache Mahoutis more mature, both Spark MLlib and Mahout include a similar set of machine-learningalgorithms. But with Mahout migrating from MapReduce to Spark, they’re bound to bemerged in the future.  Spark  MLlib  handles  machine-learning  models  used  for  transforming  datasets,which are represented as RDDs or DataFrames."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Spark GraphX\n",
    "Graphs  are  data  structures  comprising  vertices  and  the  edges  connecting  them.GraphX  provides  functions  for  building  graphs,  represented  as  graph RDDs: EdgeRDDand VertexRDD. GraphX contains implementations of the most important algorithmsof  graph  theory,  such  as  page  rank,  connected  components,  shortest  paths,  SVD++,and  others.  It  also  provides  the  Pregel  message-passing  API,  the  same  API  for  large-scale  graph  processing  implemented  by  Apache  Giraph,  a  project  with  implementa-tions of graph algorithms and running on Hadoop."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## <a id='2.2'>2.2. Spark Applications</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  <img src=\"driver_executor.png\" width=\"300px\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Spark applications consist of a **Driver Process** and a set of **executor processes**:\n",
    "\n",
    " **Driver process:**\n",
    " \n",
    "   * Run the main function.\n",
    "   * Mantain information about spark application.\n",
    "   * Responding to the user program or input. \n",
    "   * Analysing, Distributing, and scheduling work across executors.\n",
    "   \n",
    "**Executor processes:**\n",
    " \n",
    "   * Responsible for carrying out the work that driver assigns to them.\n",
    "   * Executing the code assign to it by the driver. \n",
    "   * Reporting the state of the computation on that executor back to driver node.\n",
    "   \n",
    "### Note\n",
    "There is two modes **Cluster mode** and **Local mode** In Cluster mode, Driver and executors are processes can live in the same machine or different machines. in Local mode driver and executors run(as threads) on your own computer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='2.3'>2.3. Spark Session</a>\n",
    " \n",
    "Spark SessionSession instance is the way Sparks execute user defined manipulations across the cluster.   \n",
    "To call spark Session object use `spark` command."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<pyspark.sql.session.SparkSession object at 0x115b2d710>\n"
     ]
    }
   ],
   "source": [
    "print(spark) # Spark Session Object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'spark' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-84901cd22f32>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mspark\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'spark' is not defined"
     ]
    }
   ],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a data Range of numbers:\n",
    "# This range of numbers represents a distributed collection. Each part of this \n",
    "# range of numbers exists on a different executor.\n",
    "numbers_to_n = spark.range(1000000000).toDF(\"Number\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## <a id='2.4'>2.4. Dataframes</a>\n",
    " Dataframe is the most common structed and simply represents  a table of data with rows and columns.  \n",
    " The list that defines columns and the types within those columns is called the schema.\n",
    " Spark Dataframes can span thousands of computers.  \n",
    " \n",
    " ### Note  \n",
    " Spark has multiple core abstractions: Datasets,Dataframes, SQL Tables and Resilient Distributed Datasets(RDD). These abstractions all represents distributed collections of Data.\n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "columns = ['id', 'dogs', 'cats']\n",
    "vals = [\n",
    "     (1, \"bulldog\", \"persian\"),\n",
    "     (2, \"German Shepherd\", \"Siamese\")\n",
    "]\n",
    "\n",
    "# create DataFrame\n",
    "df = spark.createDataFrame(vals, columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id: long (nullable = true)\n",
      " |-- dogs: string (nullable = true)\n",
      " |-- cats: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema() # see the schema"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## <a id='2.5'>2.5. Partitions</a>\n",
    " \n",
    " To allow every executor perform work in paralel, Spark breaks up the data into chuncks called partitions.\n",
    " Partition is a collection of rows that sits on one physical machine in your cluster. \n",
    " \n",
    " if we have multiple partitions but only one executor Spark will have a paralelism of only one because there is only one computation resource.  \n",
    " \n",
    " If we have one partitions spark will have a parallelism of only one even if we have a thousands of executors.  \n",
    " \n",
    " ### Note\n",
    " In Dataframes we don't (for the most part) manipulate partitions individualy, we simply specify high level transformations of data in the physical partitions, and spark determines how this work will actually execute on the cluster\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## <a id='2.6'>2.6. Transformations</a>\n",
    " \n",
    " In Spark core data structures are **Immutable**(cannot be changed after they're created).   \n",
    " To \"change\" a Dataframe we need to instruct spark how to modify it to do what we want. these are called transformations  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "divis_by_two = numbers_to_n.where(\"number % 2 = 0\") # why didn't return the output?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Types of transformations\n",
    "* Narrow dependencies -> each input contribute only one output partition.All performed in memory \n",
    "* Wide dependencies -> each input partitions contributing to many output partitions. Spark writes the result to disk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## <a id='2.7'>2.7. Lazy Evaluation</a>\n",
    " \n",
    " Lazy evaluation means that spark will wait until the very last moment to execute the graph of computation instructions.\n",
    " In spark we build a plan of transformations that we would like to apply to the data. By waiting until the last moment to execute the code. Spark compiles the plan and optimize the entire flow end to end."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## <a id='2.8'>2.8. Actions</a>\n",
    " \n",
    " Transformations allow us to build our logical transformation plan and trigger the computation. It's like the play button \n",
    " \n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'divis_by_two' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-5d8e6ba25e16>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdivis_by_two\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'divis_by_two' is not defined"
     ]
    }
   ],
   "source": [
    "divis_by_two.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# other action code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# other action code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# other action code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Kinds of actions:\n",
    "* Actions to view Data in the console\n",
    "* Actions to collect data to native objects in the respective language\n",
    "* Actions to write to output data sources\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## <a id='2.9'>2.9. Spark UI</a>\n",
    " With Spark UI you can monitor  the progress of a job. Usually Spark UI is available on port 4040 of the driver node.   \n",
    " Spark displays information about the state of spark jobs, its environment and cluster state.\n",
    " It is very useful for tunning and debuging."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'spark' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-16-be81978af8bc>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msparkContext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muiWebUrl\u001b[0m \u001b[0;31m# Check where spark ui is running\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'spark' is not defined"
     ]
    }
   ],
   "source": [
    "spark.sparkContext.uiWebUrl # Check where spark ui is running"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## <a id='2.10'>2.10. SQL and DataFrames</a>\n",
    " Spark can run the same transformations regredless of the language in the exact same way.  \n",
    " Spark will compile the logic to a underlying plan."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-5-02a9622b61e6>, line 3)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-5-02a9622b61e6>\"\u001b[0;36m, line \u001b[0;32m3\u001b[0m\n\u001b[0;31m    flight_data_2015 = spark.read.option.(\"inferSchema\",\"true\").option(\"header\",\"true\").csv(\"2015-summary.csv\")\u001b[0m\n\u001b[0m                                         ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "# load a dataset\n",
    "# create a dataframe \n",
    "flight_data_2015 = spark.read.option.(\"inferSchema\",\"true\").option(\"header\",\"true\").csv(\"2015-summary.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "flight_data_2015.take(3)# first three lines of the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "flight_data_2015.sort(\"count\").explain() ## check the spark physical plan of count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'spark' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-36a6f163f724>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#Spark by default has 200 shuffle output partitions.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;31m# Set partitions to 5\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"spark.sql.shuffle.partitions\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"5\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'spark' is not defined"
     ]
    }
   ],
   "source": [
    "#Spark by default has 200 shuffle output partitions. \n",
    "# Set partitions to 5\n",
    "spark.conf.set(\"spark.sql.shuffle.partitions\",\"5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a view of flight data \n",
    "flight_data_2015.createOrReplaceTempView(\"flight_data_2015\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'spark' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-871f11ed5740>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mcountry_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msql\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"\"\"SELECT DEST_COUNTRY_NAME, count(1) FROM flight_data_2015 GROUP BY DEST_COUNTRY_NAME\"\"\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'spark' is not defined"
     ]
    }
   ],
   "source": [
    "# SQL WAY\n",
    "country_name_sql = spark.sql(\"\"\"SELECT DEST_COUNTRY_NAME, count(1) FROM flight_data_2015 GROUP BY DEST_COUNTRY_NAME\"\"\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'flight_data_2015' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-34ea10c57f1b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#DATAFRAME WAYz\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mcountry_name_dataframe\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mflight_data_2015\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgroupBy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"DEST_COUNTRY_NAME\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# dataframe way\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'flight_data_2015' is not defined"
     ]
    }
   ],
   "source": [
    "#DATAFRAME WAYz\n",
    "country_name_dataframe = flight_data_2015.groupBy(\"DEST_COUNTRY_NAME\").count() # dataframe way"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "country_name_sql.explain() # same logical plan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'country_name_dataframe' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-72b164480c33>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mcountry_name_dataframe\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexplain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# same logical plan\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'country_name_dataframe' is not defined"
     ]
    }
   ],
   "source": [
    "country_name_dataframe.explain() # same logical plan"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## <a id='3'>3. Exercises</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# What are the max count from flight data?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Result output 370002"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#What are the  top 5 destinations in data?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
