{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spark Streaming & Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this lesson we will see the spark streaming module capability and classify our churn dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "- <a href='#1'>1. Context and Motivation</a>\n",
    "- <a href='#2'>2. Spark Streaming Overview</a>\n",
    "    - <a href='#2.1'>2.1. What is stream processing</a>\n",
    "    - <a href='#2.2'>2.2. Dataset</a>\n",
    "    - <a href='#2.3'>2.3. Streaming Example</a>\n",
    "    - <a href='#2.3.1'>2.3.1 Sorting and Filtering</a>\n",
    "    - <a href='#2.3.2'>2.3.2 Aggregations</a>\n",
    "- <a href='#3'>3.  Exercises</a>\n",
    "    - <a href='#3.1'>3.1. Feature Transformation</a>\n",
    "    - <a href='#3.2'>3.2. EDA</a>\n",
    "    - <a href='#3.3'>3.3. Classification</a>\n",
    "        - <a href='#3.3.1'>3.3.1 Logistic Regression</a>\n",
    "        - <a href='#3.3.2'>3.3.2 (SVM)Support vector Machine</a>\n",
    "        - <a href='#3.3.3'>3.3.3 Decision Trees</a>\n",
    "        - <a href='#3.3.4'>3.3.4 Feature Importance</a>\n",
    "    - <a href='#3.4'>3.4. Evaluation</a>\n",
    "        - <a href='#3.4.1'>3.4.1 Confusion Matrix</a>\n",
    "        - <a href='#3.4.2'>3.4.2 Accuracy</a>\n",
    "        - <a href='#3.4.3'>3.4.3 Precision</a>\n",
    "        - <a href='#3.4.4'>3.4.4 Recall</a>\n",
    "        - <a href='#3.4.6'>3.4.6 AUC(Area Under the Roc Curve)</a>\n",
    "- <a href='#4'>4.  References</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <a id='1'>1. Context and Motivation</a>\n",
    "\n",
    "Spark Streaming allow us to use realtime stream processing in applications such:\n",
    "\n",
    "* **Realtime Reporting**  \n",
    "\n",
    "* **Incremental ETL**\n",
    "\n",
    "* **Real-time decision making**\n",
    "\n",
    "* **Online machine learning**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='2.1'>2.1. What is stream processing</a>\n",
    "\n",
    "Structured Streaming enables users build **continuous applications**. A **continous application** is an end-to-end application that reacts to data in real time by combining a variety of tools: streaming jobs, batch jobs, joins between streaming and offline data, and interactive ad-hoc queries."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"Spark_streaming_output.png\" width=\"500px\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='2.2'>2.2. Dataset</a>\n",
    "\n",
    "Heterogeneity Human Activity Recognition Dataset.  \n",
    "The data consists of smartphone and **smartwatch sensor readings** from a variety of devices\n",
    "specifically, the **accelerometer and gyroscope**, sampled at the highest possible frequency\n",
    "supported by the devices.    \n",
    "Readings from these sensors were recorded while users performed\n",
    "activities like biking, sitting, standing, walking, and so on. There are several different\n",
    "smartphones and smartwatches used, and nine total users."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Put the data in **hdfs**: `hdfs dfs -put /activity_data/*`\n",
    "\n",
    "See https://hadoop.apache.org/docs/r2.4.1/hadoop-project-dist/hadoop-common/FileSystemShell.html#put"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "static = spark.read.json(\"activity-data/\") # read the dataframe as static to get the schema \n",
    "dataSchema = static.schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "streaming = spark.readStream.schema(dataSchema).option(\"maxFilesPerTrigger\", 1)\\\n",
    ".json(\"activity-data/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='2.3'>2.3. Streaming Example</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "activityCounts = streaming.groupBy(\"gt\").count()# activity being performed by the user at that point in time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.conf.set(\"spark.sql.shuffle.partitions\", 5) # avoid too many shuffle partitions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Output Modes: \n",
    "    \n",
    "* Append (only add new records to the output sink)\n",
    "* Update (update changed records in place)\n",
    "* Complete (rewrite the full output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "activityQuery = activityCounts.writeStream.queryName(\"activity_counts\")\\\n",
    ".format(\"memory\").outputMode(\"complete\")\\\n",
    ".start() # in-memory table and mode can be complete"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#activityQuery.awaitTermination() # prevent the driver process from exiting while the query is active."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.streams.active"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"SELECT * FROM activity_counts\").show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='2.3.1'>2.3.1. Selection and Filtering</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import expr\n",
    "simpleTransform = streaming.withColumn(\"stairs\", expr(\"gt like '%stairs%'\"))\\\n",
    ".where(\"stairs\")\\\n",
    ".where(\"gt is not null\")\\\n",
    ".select(\"gt\", \"model\", \"arrival_time\", \"creation_time\")\\\n",
    ".writeStream\\\n",
    ".queryName(\"simple_transform\")\\\n",
    ".format(\"memory\")\\\n",
    ".outputMode(\"append\")\\\n",
    ".start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"SELECT * FROM simple_transform\").show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='2.3.2'>2.3.2. Aggregations</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "deviceModelStats = streaming.cube(\"gt\", \"model\").avg()\\\n",
    ".drop(\"avg(Arrival_time)\")\\\n",
    ".drop(\"avg(Creation_Time)\")\\\n",
    ".drop(\"avg(Index)\")\\\n",
    ".writeStream.queryName(\"device_counts\").format(\"memory\")\\\n",
    ".outputMode(\"complete\")\\\n",
    ".start()# cube, on the phone model and activity and the average x, y, z accelerations of our sensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"SELECT * FROM device_counts\").show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <a id='3'>3. Exercises</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Customer Churn\n",
    "\n",
    "Customer churn, also known as customer attrition, customer turnover, or customer defection, is the loss of clients or customers.\n",
    "\n",
    "Telephone service companies, Internet service providers, pay TV companies, insurance firms, and alarm monitoring services, often use customer churn analysis and customer churn rates as one of their key business metrics because the cost of retaining an existing customer is far less than acquiring a new one. Companies from these sectors often have customer service branches which attempt to win back defecting clients, because recovered long-term customers can be worth much more to a company than newly recruited clients.\n",
    "\n",
    "Companies usually make a distinction between voluntary churn and involuntary churn. Voluntary churn occurs due to a decision by the customer to switch to another company or service provider, involuntary churn occurs due to circumstances such as a customer's relocation to a long-term care facility, death, or the relocation to a distant location. In most applications, involuntary reasons for churn are excluded from the analytical models. Analysts tend to concentrate on voluntary churn, because it typically occurs due to factors of the company-customer relationship which companies control, such as how billing interactions are handled or how after-sales help is provided.\n",
    "\n",
    "Predictive analytics use churn prediction models that predict customer churn by assessing their propensity of risk to churn. Since these models generate a small prioritized list of potential defectors, they are effective at focusing customer retention marketing programs on the subset of the customer base who are most vulnerable to churn."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Column Description   \n",
    "\n",
    "| Column     | Type       | Description |\n",
    "|--------  |---------  |: --------- |\n",
    "| **customerID** | String | Customer ID |\n",
    "| **gender** | String | Whether the customer is a male or a female |\n",
    "| **SeniorCitizen** | Integer | Whether the customer is a senior citizen or not (1, 0) |\n",
    "| **Partner** | String | Whether the customer has a partner or not (Yes, No) |\n",
    "| **Dependents** | String | Whether the customer has dependents or not (Yes, No) |\n",
    "| **tenure** | Integer | Number of months the customer has stayed with the company |\n",
    "| **PhoneService** | String | Whether the customer has a phone service or not (Yes, No) |\n",
    "| **MultipleLines** | String | Whether the customer has multiple lines or not (Yes, No, No phone service) |\n",
    "| **InternetService** | String | Customer’s internet service provider (DSL, Fiber optic, No) |\n",
    "| **OnlineSecurity** | String | Whether the customer has online security or not (Yes, No, No internet service) |\n",
    "| **OnlineBackup** | String | Whether the customer has online backup or not (Yes, No, No internet service) |\n",
    "| **DeviceProtection** | String | Whether the customer has device protection or not (Yes, No, No internet service) |\n",
    "| **TechSupport** | String | Whether the customer has tech support or not (Yes, No, No internet service) |\n",
    "| **StreamingTV** | String | Whether the customer has streaming movies or not (Yes, No, No internet service) |\n",
    "| **StreamingMovies** | String | Whether the customer has a partner or not (Yes, No) |\n",
    "| **Contract** | String | The contract term of the customer (Month-to-month, One year, Two year) |\n",
    "| **PaperlessBilling** | String | Whether the customer has paperless billing or not (Yes, No) |\n",
    "| **PaymentMethod** | String | The customer’s payment method (Electronic check, Mailed check, Bank transfer (automatic), Credit card (automatic)) |\n",
    "| **MonthlyCharges** | Double | The amount charged to the customer monthly |\n",
    "| **TotalCharges** | String | The total amount charged to the customer |\n",
    "| **Churn** | String | Whether the customer churned or not (Yes or No) |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='3.1'>3.1. Feature Transformation</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.format(\"csv\")\\\n",
    ".option(\"header\",\"true\")\\\n",
    ".option(\"inferSchema\",\"true\")\\\n",
    ".load(\"WA_Fn-UseC_-Telco-Customer-Churn.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import DoubleType, IntegerType\n",
    "from pyspark.sql.functions import when  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check null values\n",
    "df.where(col(\"TotalCharges\").isNull()).count() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove null values\n",
    "df = df.na.drop(subset=[\"TotalCharges\"]) ## Drop null values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import OneHotEncoder, StringIndexer\n",
    "encoder = OneHotEncoder()\\\n",
    ".setInputCol(\"gender\")\\\n",
    ".setOutputCol(\"gender\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#encoder.transform(df.select(\"gender\")).show()\n",
    "\n",
    "stringIndexer = StringIndexer(inputCol=\"gender\", outputCol=\"genderIndex\")\n",
    "model = stringIndexer.fit(df)\n",
    "indexed = model.transform(df)\n",
    "#indexed.show()\n",
    "#encoder = OneHotEncoder(inputCol=\"categoryIndex\", outputCol=\"categoryVec\")\n",
    "#encoded = encoder.transform(indexed)\n",
    "#encoded.show()\n",
    "indexed.select(\"genderIndex\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "replace_cols = [ 'OnlineSecurity', 'OnlineBackup', 'DeviceProtection',\n",
    "                'TechSupport','StreamingTV', 'StreamingMovies']\n",
    "for i in replace_cols : \n",
    "    df  = df.select(i).replace([\"No internet service\"], [\"No\"], i)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import regexp_replace, col\n",
    "replace_cols = [ 'OnlineSecurity', 'OnlineBackup', 'DeviceProtection',\n",
    "                'TechSupport','StreamingTV', 'StreamingMovies']\n",
    "for i in replace_cols :\n",
    "    df = df.withColumn(i, regexp_replace(i, \"No internet service\", \"No\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.select(\"OnlineBackup\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.select(\"gender\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.withColumn('label', when(df[\"Churn\"] == \"Yes\" , 1).otherwise(0)) # convert into 0 or 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import RFormula\n",
    "supervised = RFormula(formula=\"label ~ . + Churn:TotalCharges + Churn:MonthlyCharges + Churn:SeniorCitizen\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fittedRF = supervised.fit(df)\n",
    "preparedDF = fittedRF.transform(df)\n",
    "preparedDF.show(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, test = preparedDF.randomSplit([0.7, 0.3]) ## preparing dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='3.2'>3.2. EDA</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='3.3'>3.3. Classification</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='3.3.1'>3.3.1 Logistic Regression</a>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import LogisticRegression\n",
    "\n",
    "lr = LogisticRegression(labelCol=\"label\",featuresCol=\"features\",  regParam=0.3, elasticNetParam=0.8)\n",
    "\n",
    "# Fit the model\n",
    "lrModel = lr.fit(train)\n",
    "\n",
    "# Print the coefficients and intercept for logistic regression\n",
    "print(\"Coefficients: \" + str(lrModel.coefficients))\n",
    "print(\"Intercept: \" + str(lrModel.intercept))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='3.3.2'>3.3.2 (SVM)Support vector Machine</a> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Create a model with support vector machine algoritm**   \n",
    "See https://spark.apache.org/docs/latest/ml-classification-regression.html#linear-support-vector-machine"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='3.3.3'>3.3.3 Decision Trees</a>\n",
    "**Create a model with DecisionTreeClassifier algoritm**   \n",
    "See https://spark.apache.org/docs/latest/ml-classification-regression.html#linear-support-vector-machine"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='3.3.4'>3.3.4 Feature importance</a>\n",
    "**Using the decision tree algorithm plot the tree and see what are the most important features that define the model.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='3.4'>3.4 Evaluation</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainingSummary = lrModel.summary # model summary to get the metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='3.4.1'>3.4.1. Confusion Matrix</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"confusion_matrix.png\" width=\"350px\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* True Positive: -> Interpretation: You predicted positive and it’s true.\n",
    "* True Negative: -> Interpretation: You predicted negative and it’s true.\n",
    "* False Positive: -> Interpretation: You predicted positive and it’s false.\n",
    "* False Negative: -> Interpretation: You predicted negative and it’s false."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='3.4.2'>3.4.2. Accuracy</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Accuracy = (TP+TN)/(TP+FP+FN+TN)** -> How many churners did we correctly label out of all the churners"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy = trainingSummary.accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Accuracy:\"+ str(accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# What are the accuracy for the other modules??"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='3.4.3'>3.4.3. Precision</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Precision = TP/(TP+FP)**  -> How many of those who we labeled as churners are actually churners"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "precision = trainingSummary.weightedPrecision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Precision:\"+ str(precision))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# What are the precision for the other modules??"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='3.4.4'>3.4.4. Recall</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Recall = TP/(TP+FN)** -> Of all the people who are churners, how many of those we correctly predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "recall = trainingSummary.weightedRecall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Recall:\"+ str(precision))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# What are the Recall for the other modules??"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='3.4.4'>3.4.4. AUC(Area Under the Roc Curve)</a>\n",
    "\n",
    "See https://towardsdatascience.com/understanding-auc-roc-curve-68b2303cc9c5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"areaUnderROC: \" + str(trainingSummary.areaUnderROC))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainingSummary.roc.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='4'>4. References</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://github.com/databricks/Spark-The-Definitive-Guide"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://en.wikipedia.org/wiki/Confusion_matrix"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
