{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Agreggations, Joins & Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this lesson we will create aggregations, joins and classify a churn dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "- <a href='#1'>1. Context and Motivation</a>\n",
    "- <a href='#2'>2. Agreggations</a>\n",
    "    - <a href='#2.1'>2.1. Aggregation Functions</a>\n",
    "    - <a href='#2.2'>2.2. Grouping</a>\n",
    "    - <a href='#2.3'>2.3. Window Functions</a>\n",
    "    - <a href='#2.4'>2.4. User-Defined Aggregation Functions</a>\n",
    "- <a href='#2'>3. Joins</a>\n",
    "    - <a href='#3.1'>3.1. Join Types</a>\n",
    "        - <a href='#3.1.1'>3.1.1 Inner Joins</a>\n",
    "        - <a href='#3.1.2'>3.1.2 Outer Joins</a>\n",
    "        - <a href='#3.1.3'>3.1.3 Left Outer Joins</a>\n",
    "        - <a href='#3.1.4'>3.1.4 Right Outer Joins</a>\n",
    "        - <a href='#3.1.5'>3.1.5 Left Semi Joins</a>\n",
    "        - <a href='#3.1.6'>3.1.6 Left Anti Joins</a>\n",
    "        - <a href='#3.1.7'>3.1.7 Natural Joins</a>\n",
    "        - <a href='#3.1.8'>3.1.8 Cross (Cartesian) Joins</a>\n",
    "    - <a href='#3.2'>3.3. How Spark Perform Joins</a>\n",
    "- <a href='#4'>4.  Exercises</a>\n",
    "    - <a href='#4.1'>4.1. EDA</a>\n",
    "    - <a href='#4.2'>4.2. Classification</a>\n",
    "        - <a href='#4.2.1'>4.2.1 Logistic Regression</a>\n",
    "        - <a href='#4.2.2'>4.2.2 (SVM)Support vector Machine</a>\n",
    "        - <a href='#4.2.3'>4.2.3 Decision Trees</a>\n",
    "        - <a href='#4.2.4'>4.2.4 Feature Importance</a>\n",
    "    - <a href='#4.3'>4.3. Evaluation</a>\n",
    "- <a href='#5'>5.  References</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <a id='1'>1. Context and Motivation</a>\n",
    "\n",
    "When we work with data we need to transform the data to get into something that we need to view. These transformations always come with **Agreggations**. \n",
    "\n",
    "\n",
    "Sometimes we need to perform some dataset joins in order to join multiple datasets to have more peformance we need to know which **join** we use. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <a id='2'>2. Agreggations</a>\n",
    "\n",
    "Aggregation is an act of collecting something together, we will specify a key or grouping and an aggregation function that specifies how we should transform one or more columns.   \n",
    "We need to perform aggregations to transform one or more columns, grouping data and view the data the way we want. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.format(\"csv\")\\\n",
    ".option(\"header\", \"true\")\\\n",
    ".option(\"inferSchema\", \"true\")\\\n",
    ".load(\"retail_data_2010-12-01.csv\")\\\n",
    "\n",
    "df.cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='2.1'>2.1. Aggregation Functions</a>\n",
    "\n",
    "All aggregation functions are available as functions and we can find the most of them in: https://spark.apache.org/docs/latest/api/python/pyspark.sql.html#module-pyspark.sql.functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function **count(col)** \n",
    "* Aggregate function - Returns the number of items in a group.\n",
    "* See https://spark.apache.org/docs/latest/api/python/pyspark.sql.html#module-pyspark.sql.functions.\n",
    "* IF count(*) count nulls either."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import count\n",
    "df.select(count(\"StockCode\")).show() # 541909"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function **countDistinct(col, *cols)** \n",
    "* Aggregate function - Returns a new Column for distinct count of col or cols.\n",
    "* See https://spark.apache.org/docs/latest/api/python/pyspark.sql.html#module-pyspark.sql.functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import countDistinct\n",
    "df.select(countDistinct(\"StockCode\")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function **approx_count_distinct(col, rsd=None)** \n",
    "* Aggregate function: returns a new Column for approximate distinct count of column col.\n",
    "* See https://spark.apache.org/docs/latest/api/python/pyspark.sql.html#module-pyspark.sql.functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import approx_count_distinct\n",
    "df.select(approx_count_distinct(\"StockCode\", 0.1)).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function **first(col, ignorenulls=False)** \n",
    "* Aggregate function: returns the first value in a group.\n",
    "* See https://spark.apache.org/docs/latest/api/python/pyspark.sql.html#module-pyspark.sql.functions\n",
    "\n",
    "Function **last(col, ignorenulls=False)** \n",
    "* Aggregate function: returns the last value in a group.\n",
    "* See https://spark.apache.org/docs/latest/api/python/pyspark.sql.html#module-pyspark.sql.functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import first, last\n",
    "df.select(first(\"StockCode\"), last(\"StockCode\")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function **min(col)** \n",
    "* Aggregate function: returns the minimum value of the expression in a group.\n",
    "* See https://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.functions.min\n",
    "\n",
    "Function **max(col)** \n",
    "* Aggregate function: returns the maximum value of the expression in a group.\n",
    "* See https://spark.apache.org/docs/latest/api/python/pyspark.sql.html#module-pyspark.sql.max"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import min, max\n",
    "df.select(min(\"Quantity\"), max(\"Quantity\")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function **sum(col)** \n",
    "* Aggregate function: returns the sum of all values in the expression.\n",
    "* See https://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.functions.sum\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import sum\n",
    "df.select(sum(\"Quantity\")).show() # 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function **sumDistinct(col)** \n",
    "* Aggregate function: returns the sum of distinct values in the expression.\n",
    "* See https://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.functions.sumDistinct\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import sumDistinct\n",
    "df.select(sumDistinct(\"Quantity\")).show() # 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function **avg(col)** \n",
    "* Aggregate function: returns the average of the values in a group.\n",
    "* See https://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.functions.avg\n",
    "\n",
    "Function **expr(str)** \n",
    "* Parses the expression string into the column that it represents.\n",
    "* See https://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.functions.expr\n",
    "\n",
    "Function **sum(col)** \n",
    "* Aggregate function: returns the sum of all values in the expression.\n",
    "* See https://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.functions.sum\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import sum, count, avg, expr\n",
    "df.select(\n",
    "count(\"Quantity\").alias(\"total_transactions\"),\n",
    "sum(\"Quantity\").alias(\"total_purchases\"),\n",
    "avg(\"Quantity\").alias(\"avg_purchases\"),\n",
    "expr(\"mean(Quantity)\").alias(\"mean_purchases\"))\\\n",
    ".selectExpr(\n",
    "\"total_purchases/total_transactions\",\n",
    "\"avg_purchases\",\n",
    "\"mean_purchases\").show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function **var_pop(col)** \n",
    "* Aggregate function: returns the population variance of the values in a group.\n",
    "* See https://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.functions.var_pop\n",
    "\n",
    "Function **stddev_pop(col)** \n",
    "* Aggregate function: returns population standard deviation of the expression in a group.\n",
    "* See https://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.functions.stddev_pop\n",
    "\n",
    "Function **var_samp(col)** \n",
    "* Aggregate function: returns the unbiased sample variance of the values in a group.\n",
    "* See https://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.functions.var_samp\n",
    "\n",
    "Function **stddev_samp(col)** \n",
    "* Aggregate function: returns the unbiased sample standard deviation of the expression in a group.\n",
    "* See https://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.functions.stddev_samp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import var_pop, stddev_pop\n",
    "from pyspark.sql.functions import var_samp, stddev_samp\n",
    "df.select(var_pop(\"Quantity\"), var_samp(\"Quantity\"),\n",
    "stddev_pop(\"Quantity\"), stddev_samp(\"Quantity\")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Aggregating to Complex Types\n",
    "\n",
    "Function **agg(col)** \n",
    "* Aggregate on the entire DataFrame without groups (shorthand for df.groupBy.agg()).\n",
    "* See https://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.DataFrame.agg \n",
    "\n",
    "Function **collect_set(col)** \n",
    "* Aggregate function: returns a set of objects with duplicate elements eliminated.\n",
    "* See https://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.functions.collect_set\n",
    "\n",
    "Function **collect_list(col)** \n",
    "* Aggregate function: returns a list of objects with duplicates.\n",
    "* See https://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.functions.collect_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import collect_set, collect_list\n",
    "df.agg(collect_set(\"Country\"), collect_list(\"Country\")).show(20,True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import  collect_list\n",
    "df.agg(collect_set(\"Country\")).show(20,False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import  collect_list\n",
    "df.agg(collect_list(\"Country\")).show(1,True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='2.2'>2.2. Grouping</a>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function **groupby(*cols)** \n",
    "* Groups the DataFrame using the specified columns, so we can run aggregation on them.\n",
    "* See https://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.DataFrame.groupBy "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.groupBy(\"InvoiceNo\", \"CustomerId\").count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import count\n",
    "df.groupBy(\"InvoiceNo\").agg(\n",
    "count(\"Quantity\").alias(\"quantit\"),\n",
    "expr(\"count(Quantity)\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.groupBy(\"InvoiceNo\").agg(expr(\"avg(Quantity)\"),expr(\"stddev_pop(Quantity)\"))\\\n",
    ".show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='2.3'>2.3. Window Functions</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function **dense_rank(*cols)** \n",
    "* Window function: returns the rank of rows within a window partition, without any gaps.\n",
    "* See https://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.DataFrame.dense_rank \n",
    "    \n",
    "Function **rank(*cols)** \n",
    "* Window function: returns the rank of rows within a window partition.\n",
    "* See https://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.functions.rank \n",
    "\n",
    "We are going to establishing the maximum purchase quantity over all time. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, to_date,desc, max, dense_rank, rank\n",
    "from pyspark.sql.window import Window\n",
    "\n",
    "dfWithDate = df.withColumn(\"date\", to_date(col(\"InvoiceDate\"), \"MM/d/yyyy H:mm\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "windowSpec = Window\\\n",
    ".partitionBy(\"CustomerId\", \"date\")\\\n",
    ".orderBy(desc(\"Quantity\"))\\\n",
    ".rowsBetween(Window.unboundedPreceding, Window.currentRow)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "maxPurchaseQuantity = max(col(\"Quantity\")).over(windowSpec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "purchaseDenseRank = dense_rank().over(windowSpec)\n",
    "purchaseRank = rank().over(windowSpec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfWithDate.where(\"CustomerId IS NOT NULL\").orderBy(\"CustomerId\")\\\n",
    ".select(\n",
    "col(\"CustomerId\"),\n",
    "col(\"date\"),\n",
    "col(\"Quantity\"),\n",
    "purchaseRank.alias(\"quantityRank\"),\n",
    "purchaseDenseRank.alias(\"quantityDenseRank\"),\n",
    "maxPurchaseQuantity.alias(\"maxPurchaseQuantity\")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='2.4'>2.4. User-Defined Aggregation Functions</a>\n",
    "\n",
    "**Can be define but only in Java or Scala.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <a id='3'>3. Joins</a>\n",
    "A join brings together two sets of data, the left and the right, by comparing the value of one or\n",
    "more keys of the left and right and <evaluating the result of a join expression that determines\n",
    "whether Spark should bring together the left set of data with the right set of data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='3.1'>3.1. Join Types</a>\n",
    "Whereas the join expression determines whether two rows should join, the join type determines\n",
    "what should be in the result set. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "person = spark.createDataFrame([\n",
    "(0, \"Bill Chambers\", 0, [100]),\n",
    "(1, \"Matei Zaharia\", 1, [500, 250, 100]),\n",
    "(2, \"Michael Armbrust\", 1, [250, 100])])\\\n",
    ".toDF(\"id\", \"name\", \"graduate_program\", \"spark_status\")\n",
    "\n",
    "graduateProgram = spark.createDataFrame([\n",
    "(0, \"Masters\", \"School of Information\", \"ISCTE\"),\n",
    "(2, \"Masters\", \"School of Information\", \"ISCTE\"),\n",
    "(1, \"Ph.D.\", \"School of Information\", \"ISCTE\")])\\\n",
    ".toDF(\"id\", \"degree\", \"department\", \"school\")\n",
    "\n",
    "sparkStatus = spark.createDataFrame([\n",
    "(500, \"Vice President\"),\n",
    "(250, \"PMC Member\"),\n",
    "(100, \"Contributor\")])\\\n",
    ".toDF(\"id\", \"status\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='3.1.1'>3.1.1. Inner Joins</a>\n",
    "Inner joins evaluate the keys in both of the DataFrames or tables and include (and join together)\n",
    "only the rows that evaluate to true.   \n",
    "**Inner joins (keep rows with keys that exist in the left and right datasets)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "joinExpression = person[\"graduate_program\"] == graduateProgram['id']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "joinExpression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#newJoinExpression = person[\"name\"] == graduateProgram[\"school\"] This will work?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### join in dataframe "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "person.join(graduateProgram, joinExpression).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "joinType = \"inner\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "person.join(graduateProgram, joinExpression, joinType).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='3.1.2'>3.1.2. Outer Joins</a>\n",
    "Outer joins evaluate the keys in both of the DataFrames or tables and includes (and joins\n",
    "together) the rows that evaluate to true or false. If there is no equivalent row in either the left or\n",
    "right DataFrame, Spark will insert null:\n",
    "\n",
    "**Outer joins (keep rows with keys in either the left or right datasets)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "joinType = \"outer\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "person.join(graduateProgram, joinExpression, joinType).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='3.1.3'>3.1.3. Left Outer Joins</a>\n",
    "\n",
    "Left outer joins evaluate the keys in both of the DataFrames or tables and includes all rows from\n",
    "the left DataFrame as well as any rows in the right DataFrame that have a match in the left\n",
    "DataFrame.   \n",
    "\n",
    "**Left outer joins (keep rows with keys in the left dataset)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "joinType = \"left_outer\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "graduateProgram.join(person, joinExpression, joinType).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='3.1.4'>3.1.4. Right Outer Joins</a>\n",
    "\n",
    "Right outer joins evaluate the keys in both of the DataFrames or tables and includes all rows\n",
    "from the right DataFrame as well as any rows in the left DataFrame that have a match in the right\n",
    "DataFrame.\n",
    "\n",
    "**Right outer joins (keep rows with keys in the right dataset)**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "joinType = \"right_outer\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "person.join(graduateProgram, joinExpression, joinType).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###   <a id='3.1.5'>3.1.5. Left Semi Joins</a>\n",
    "\n",
    "Semi joins are a bit of a departure from the other joins. They do not actually include any values\n",
    "from the right DataFrame. They only compare values to see if the value exists in the second\n",
    "DataFrame. If the value does exist, those rows will be kept in the result, even if there are\n",
    "duplicate keys in the left DataFrame. \n",
    "\n",
    "\n",
    "**Left semi joins (keep the rows in the left, and only the left, dataset where the key appears in the right dataset)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "joinType = \"left_semi\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "graduateProgram.join(person, joinExpression, joinType).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gradProgram2 = graduateProgram.union(spark.createDataFrame([\n",
    "(0, \"Masters\", \"Duplicated Row\", \"Duplicated School\")]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gradProgram2.join(person, joinExpression, joinType).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  <a id='3.1.6'>3.1.6. Left Anti Joins</a>\n",
    "\n",
    "Left anti joins are the opposite of left semi joins. Like left semi joins, they do not actually\n",
    "include any values from the right DataFrame.    \n",
    "They only compare values to see if the value exists in the second DataFrame.   \n",
    "However, rather than keeping the values that exist in the second\n",
    "DataFrame, they keep only the values that do not have a corresponding key in the second\n",
    "DataFrame.\n",
    "\n",
    "**Left anti joins (keep the rows in the left, and only the left, dataset where they do not appear in the right dataset)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "joinType = \"left_anti\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "graduateProgram.join(person, joinExpression, joinType).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='3.1.7'>3.1.7.  Natural Joins</a>\n",
    "\n",
    "Natural joins make implicit guesses at the columns on which you would like to join. It finds\n",
    "matching columns and returns the results. Left, right, and outer natural joins are all supported.\n",
    "\n",
    "**Natural joins (perform a join by implicitly matching the columns between the two datasets with the same names)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='3.1.8'>3.1.8.  Cross (Cartesian) Joins</a>\n",
    "\n",
    "The last of our joins are cross-joins or cartesian products. Cross-joins in simplest terms are inner\n",
    "joins that do not specify a predicate. Cross joins will join every single row in the left DataFrame\n",
    "to ever single row in the right DataFrame\n",
    "\n",
    "\n",
    "**Cross (or Cartesian) joins (match every row in the left dataset with every row in the right dataset)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "joinType = \"cross\"\n",
    "graduateProgram.join(person, joinExpression, joinType).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "person.crossJoin(graduateProgram).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='3.2'>3.2. How Spark Perform Joins</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Big table–to–big table\n",
    "When you join a big table to another big table, you end up with a shuffle join, such as that"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"big-to-big.png\" width=\"500px\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Big table–to–small table\n",
    "When the table is small enough to fit into the memory of a single worker node, we can optimize our join. Although we can use a big table–to–big table communication strategy, it can often be more efficient to use a broadcast join. What this means is that we will replicate our small DataFrame onto every worker node in the cluster (be it\n",
    "located on one machine or many). Now this sounds expensive. However, what this does is\n",
    "prevent us from performing the all-to-all communication during the entire join process. Instead,\n",
    "we perform it only once at the beginning and then let each individual worker node perform the\n",
    "work without having to wait or communicate with any other worker node."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"big-to-small.png\" width=\"700px\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import broadcast \n",
    "\n",
    "person.join(broadcast(graduateProgram), joinExpr).explain() # Marks a DataFrame as small enough for use in broadcast joins."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Little table–to–little table\n",
    "When performing joins with small tables, it’s usually best to let Spark decide how to join them.   \n",
    "You can always force a broadcast join if you’re noticing strange behavior."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <a id='4'>4. Exercises</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Customer Churn\n",
    "\n",
    "Customer churn, also known as customer attrition, customer turnover, or customer defection, is the loss of clients or customers.\n",
    "\n",
    "Telephone service companies, Internet service providers, pay TV companies, insurance firms, and alarm monitoring services, often use customer churn analysis and customer churn rates as one of their key business metrics because the cost of retaining an existing customer is far less than acquiring a new one. Companies from these sectors often have customer service branches which attempt to win back defecting clients, because recovered long-term customers can be worth much more to a company than newly recruited clients.\n",
    "\n",
    "Companies usually make a distinction between voluntary churn and involuntary churn. Voluntary churn occurs due to a decision by the customer to switch to another company or service provider, involuntary churn occurs due to circumstances such as a customer's relocation to a long-term care facility, death, or the relocation to a distant location. In most applications, involuntary reasons for churn are excluded from the analytical models. Analysts tend to concentrate on voluntary churn, because it typically occurs due to factors of the company-customer relationship which companies control, such as how billing interactions are handled or how after-sales help is provided.\n",
    "\n",
    "Predictive analytics use churn prediction models that predict customer churn by assessing their propensity of risk to churn. Since these models generate a small prioritized list of potential defectors, they are effective at focusing customer retention marketing programs on the subset of the customer base who are most vulnerable to churn."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Column Description   \n",
    "\n",
    "| Column     | Type       | Description |\n",
    "|--------  |---------  |: --------- |\n",
    "| **customerID** | String | Customer ID |\n",
    "| **gender** | String | Whether the customer is a male or a female |\n",
    "| **SeniorCitizen** | Integer | Whether the customer is a senior citizen or not (1, 0) |\n",
    "| **Partner** | String | Whether the customer has a partner or not (Yes, No) |\n",
    "| **Dependents** | String | Whether the customer has dependents or not (Yes, No) |\n",
    "| **tenure** | Integer | Number of months the customer has stayed with the company |\n",
    "| **PhoneService** | String | Whether the customer has a phone service or not (Yes, No) |\n",
    "| **MultipleLines** | String | Whether the customer has multiple lines or not (Yes, No, No phone service) |\n",
    "| **InternetService** | String | Customer’s internet service provider (DSL, Fiber optic, No) |\n",
    "| **OnlineSecurity** | String | Whether the customer has online security or not (Yes, No, No internet service) |\n",
    "| **OnlineBackup** | String | Whether the customer has online backup or not (Yes, No, No internet service) |\n",
    "| **DeviceProtection** | String | Whether the customer has device protection or not (Yes, No, No internet service) |\n",
    "| **TechSupport** | String | Whether the customer has tech support or not (Yes, No, No internet service) |\n",
    "| **StreamingTV** | String | Whether the customer has streaming movies or not (Yes, No, No internet service) |\n",
    "| **StreamingMovies** | String | Whether the customer has a partner or not (Yes, No) |\n",
    "| **Contract** | String | The contract term of the customer (Month-to-month, One year, Two year) |\n",
    "| **PaperlessBilling** | String | Whether the customer has paperless billing or not (Yes, No) |\n",
    "| **PaymentMethod** | String | The customer’s payment method (Electronic check, Mailed check, Bank transfer (automatic), Credit card (automatic)) |\n",
    "| **MonthlyCharges** | Double | The amount charged to the customer monthly |\n",
    "| **TotalCharges** | String | The total amount charged to the customer |\n",
    "| **Churn** | String | Whether the customer churned or not (Yes or No) |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create the dataframe.\n",
    "df = spark.read.format(\"csv\")\\\n",
    ".option(\"header\",\"true\")\\\n",
    ".option(\"inferSchema\",\"true\")\\\n",
    ".load(\"WA_Fn-UseC_-Telco-Customer-Churn.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='4.1'>4.1. EDA (Exploratory Data Analysis)</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Convert types in string into numbers where is possible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import DoubleType, IntegerType\n",
    "from pyspark.sql.functions import when  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.withColumn(\"TotalCharges\", df[\"TotalCharges\"].cast(DoubleType()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.withColumn('Label', when(df[\"Churn\"] == \"Yes\" , 1).otherwise(0)) # convert into 0 or 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.where(col(\"TotalCharges\").isNull()).count() # check null values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.where(col(\"SeniorCitizen\").isNull()).count() # check null values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.na.drop(subset=[\"TotalCharges\"]) ## Drop null values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='4.2'>4.2.Classification</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='4.2.1'>4.2.1. Logistic Regression</a>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import RFormula\n",
    "supervised = RFormula(formula=\"label ~ . + Churn:TotalCharges + Churn:MonthlyCharges + Churn:SeniorCitizen\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fittedRF = supervised.fit(df)\n",
    "preparedDF = fittedRF.transform(df)\n",
    "preparedDF.show(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exercise: **Create a logistic regression model**  \n",
    "See https://spark.apache.org/docs/latest/ml-classification-regression.html#logistic-regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import LogisticRegression\n",
    "\n",
    "train, test = preparedDF.randomSplit([0.7, 0.3])\n",
    "\n",
    "lr = LogisticRegression(labelCol=\"Label\",featuresCol=\"features\",  regParam=0.3, elasticNetParam=0.8)\n",
    "\n",
    "# Fit the model\n",
    "lrModel = lr.fit(train)\n",
    "\n",
    "# Print the coefficients and intercept for logistic regression\n",
    "print(\"Coefficients: \" + str(lrModel.coefficients))\n",
    "print(\"Intercept: \" + str(lrModel.intercept))\n",
    "\n",
    "# We can also use the multinomial family for binary classification\n",
    "mlr = LogisticRegression(labelCol=\"Label\",featuresCol=\"features\", maxIter=10, regParam=0.3, elasticNetParam=0.8, family=\"multinomial\")\n",
    "\n",
    "# Fit the model\n",
    "mlrModel = mlr.fit(train)\n",
    "\n",
    "# Print the coefficients and intercepts for logistic regression with multinomial family\n",
    "print(\"Multinomial coefficients: \" + str(mlrModel.coefficientMatrix))\n",
    "print(\"Multinomial intercepts: \" + str(mlrModel.interceptVector))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='4.2.2'>4.2.2. Support Vector Machine</a>\n",
    "Create a Suport Vector machine Classification.   \n",
    "See https://spark.apache.org/docs/latest/ml-classification-regression.html#linear-support-vector-machine"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='4.2.3'>4.2.3. Decision Trees </a>\n",
    "Create a Decision Tree Classification.\n",
    "See https://spark.apache.org/docs/latest/ml-classification-regression.html#decision-tree-classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='4.2.4'>4.2.4. Feature importance </a>\n",
    "\n",
    "Print the tree and check the most important features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='4.3'>4.3. Evaluation</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract the summary from the returned LogisticRegressionModel instance trained\n",
    "# in the earlier example\n",
    "trainingSummary = mlrModel.summary\n",
    "\n",
    "# Obtain the objective per iteration\n",
    "objectiveHistory = trainingSummary.objectiveHistory\n",
    "print(\"objectiveHistory:\")\n",
    "for objective in objectiveHistory:\n",
    "    print(objective)\n",
    "\n",
    "trainingSummary.roc.show()\n",
    "print(\"areaUnderROC: \" + str(trainingSummary.areaUnderROC))\n",
    "\n",
    "# Set the model threshold to maximize F-Measure\n",
    "fMeasure = trainingSummary.fMeasureByThreshold\n",
    "maxFMeasure = fMeasure.groupBy().max('F-Measure').select('max(F-Measure)').head()\n",
    "bestThreshold = fMeasure.where(fMeasure['F-Measure'] == maxFMeasure['max(F-Measure)']) \\\n",
    "    .select('threshold').head()['threshold']\n",
    "lr.setThreshold(bestThreshold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"False positive rate by label:\")\n",
    "for i, rate in enumerate(trainingSummary.falsePositiveRateByLabel):\n",
    "    print(\"label %d: %s\" % (i, rate))\n",
    "\n",
    "print(\"True positive rate by label:\")\n",
    "for i, rate in enumerate(trainingSummary.truePositiveRateByLabel):\n",
    "    print(\"label %d: %s\" % (i, rate))\n",
    "\n",
    "print(\"Precision by label:\")\n",
    "for i, prec in enumerate(trainingSummary.precisionByLabel):\n",
    "    print(\"label %d: %s\" % (i, prec))\n",
    "\n",
    "print(\"Recall by label:\")\n",
    "for i, rec in enumerate(trainingSummary.recallByLabel):\n",
    "    print(\"label %d: %s\" % (i, rec))\n",
    "\n",
    "print(\"F-measure by label:\")\n",
    "for i, f in enumerate(trainingSummary.fMeasureByLabel()):\n",
    "    print(\"label %d: %s\" % (i, f))\n",
    "\n",
    "accuracy = trainingSummary.accuracy\n",
    "falsePositiveRate = trainingSummary.weightedFalsePositiveRate\n",
    "truePositiveRate = trainingSummary.weightedTruePositiveRate\n",
    "fMeasure = trainingSummary.weightedFMeasure()\n",
    "precision = trainingSummary.weightedPrecision\n",
    "recall = trainingSummary.weightedRecall"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='4.3.1'>4.3.1. Confusion Matrix</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='4.3.1'>4.3.1. AUC</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <a id='5'>5. References</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://www.oreilly.com/library/view/spark-the-definitive/9781491912201/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://spark.apache.org/docs/latest/api/python/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://spark.apache.org/docs/latest/ml-classification-regression.html#logistic-regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://spark.apache.org/docs/latest/ml-classification-regression.html#logistic-regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://spark.apache.org/docs/latest/ml-classification-regression.html#decision-tree-classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://spark.apache.org/docs/latest/ml-classification-regression.html#linear-support-vector-machine"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
